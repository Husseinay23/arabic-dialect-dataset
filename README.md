 # ğŸ—£ï¸ Arabic Dialect Speech Dataset Pipeline

This repository contains a complete pipeline to create a clean, labeled, and ready-to-train dataset of Arabic dialect speech audio samples extracted from YouTube. It supports multiple dialects and generates Mel spectrograms to be used in deep learning models.

---

## ğŸŒ Supported Dialects

The pipeline is designed to support any number of dialects. It currently includes:

- ğŸ‡±ğŸ‡§ Lebanese
- ğŸ‡ªğŸ‡¬ Egyptian
- ğŸ‡¯ğŸ‡´ Jordanian
- ğŸ‡¸ğŸ‡¾ Syrian
- ğŸ‡®ğŸ‡¶ Iraqi
- ğŸ‡µğŸ‡¸ Palestinian
- ğŸ‡¸ğŸ‡¦ Saudi
- ğŸ‡¦ğŸ‡ª Emirati

Each dialect has its own folder and is processed independently.

---

## ğŸ› ï¸ Pipeline Overview

### 1. ğŸ§ YouTube Audio Downloader
Downloads full audio from YouTube videos using `yt-dlp`. Files are saved in a `full_audio/` folder per dialect.

```python
download_youtube_audio("https://youtube.com/...", dialect_dir="Dataset/Lebanese")
````

* Format: `.mp3`
* Trims the first 60 seconds to skip intros

---

### 2. ğŸ§  Voice Activity Detection (VAD)

Applies WebRTC VAD to extract clean voice chunks (default 7 seconds each). Adds padding before and after speech.

```python
segments = vad_collector(audio)
```

* Removes silence and background noise
* Segments are saved as `.wav` files
* Each dialect has its own CSV metadata file

---

### 3. ğŸ“„ Metadata Creation

Each saved segment is labeled with:

* `sample_id`
* `filename`
* `dialect`
* `start_time_ms`, `end_time_ms`
* `start_time_str`, `end_time_str`
* `source_url`

**Example metadata row:**

```csv
sample_id,filename,dialect,duration_sec,start_time_ms,end_time_ms,start_time_str,end_time_str,source_url
abc12345,lebanese_chunk_0001.wav,Lebanese,6.95,61000,68000,01:01,01:08,https://youtube.com/...
```

---

### 4. ğŸ“¦ Dataset Split

Automatically splits each dialect dataset into:

* `train` (70%)
* `val` (15%)
* `test` (15%)

Files are **moved** to:

```
Dataset/
â””â”€â”€ Lebanese/
    â”œâ”€â”€ train/wav/
    â”œâ”€â”€ val/wav/
    â””â”€â”€ test/wav/
```

The metadata CSV is updated with a `split` column.

---

### 5. ğŸ¼ Mel Spectrogram Generation

Generates Mel spectrograms (`.png`) from `.wav` files using Librosa and Matplotlib.

```python
mel = librosa.feature.melspectrogram(y, sr=16000, n_mels=128)
mel_db = librosa.power_to_db(mel, ref=np.max)
```

* Output saved to:

  ```
  Dataset/Lebanese/train/mel/lebanese_chunk_0001.png
  ```
* Metadata CSV is updated with `mel_path`



---

## ğŸ“ Final Folder Structure

```
Dataset/
â”œâ”€â”€ Lebanese/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ wav/
â”‚   â”‚   â””â”€â”€ mel/
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ wav/
â”‚   â”‚   â””â”€â”€ mel/
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”œâ”€â”€ wav/
â”‚   â”‚   â””â”€â”€ mel/
â”‚   â””â”€â”€ Lebanese_metadata.csv
â”œâ”€â”€ Egyptian/
â”‚   â””â”€â”€ ...
```

---

## â–¶ï¸ How to Run

Run the full pipeline in a Jupyter Notebook using:

1. `process_multiple_youtube_links(dialect, links)`
2. `split_all_dialects(base_dir="./Dataset")`
3. `generate_mel_spectrograms(dialect_dir, dialect)`

Or loop through all dialects:

```python
dialects = ['Lebanese', 'Egyptian', 'Jordanian', 'Syrian', 'Iraqi', 'Palestinian', 'Saudi', 'Emirati']
for dialect in dialects:
    generate_mel_spectrograms(os.path.join('./Dataset', dialect), dialect)
```

---

## ğŸ“¦ Dependencies

Install with pip:

```bash
pip install -r requirements.txt
```

**Main libraries:**

* `yt-dlp` (for YouTube downloads)
* `pydub` (audio processing)
* `webrtcvad` (voice activity detection)
* `librosa`, `matplotlib`, `numpy`, `pandas` (for spectrograms and metadata)
* `scikit-learn` (for splitting)

---

## ğŸš€ Future Improvements

* [ ] Save spectrograms as `.npy` arrays instead of `.png`
* [ ] Add automatic dialect detection or label verification
* [ ] Include command-line interface (CLI) for automation
* [ ] Use Whisper or DeepSpeech to align transcription (optional)

---

## ğŸ™Œ Acknowledgments

This project was developed by Hussein Ayoub for academic and research purposes in Arabic speech processing and dialect AI modeling. Data is sourced from publicly available YouTube videos 
