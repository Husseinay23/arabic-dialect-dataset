{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9022460-2515-4938-b94a-c0d8702f1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Creation Pipeline\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import csv\n",
    "import subprocess\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import webrtcvad\n",
    "import whisper\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------\n",
    "CHUNK_DURATION_MS = 7000                   # Desired chunk length\n",
    "SILENCE_THRESHOLD_DB = -40                 # Unused, but reserved for silence logic\n",
    "VAD_MODE = 2                               # WebRTC VAD aggressiveness (0 = loose, 3 = strict)\n",
    "TARGET_DBFS = -20.0                        # Normalize loudness\n",
    "SAMPLE_RATE = 16000                        # Audio sample rate (Hz)\n",
    "CHANNELS = 1                               # Mono channel\n",
    "PADDING_MS = 500                           # Padding before/after detected speech (ms)\n",
    "MAX_AUDIO_DURATION_MIN = 30                # Cap audio length to first 30 minutes\n",
    "WHISPER_MODEL_SIZE = \"base\"                # Whisper model size (\"base\", \"small\", \"medium\")\n",
    "OUTPUT_BASE = \"./Dataset\"                  # Base output directory\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD WHISPER MODEL ONCE\n",
    "# ---------------------------\n",
    "print(f\"🧠 Loading Whisper model: {WHISPER_MODEL_SIZE}...\")\n",
    "model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "\n",
    "# ---------------------------\n",
    "# Download YouTube Audio\n",
    "# ---------------------------\n",
    "def download_youtube_audio(youtube_url, output_dir, filename=\"full_audio.mp3\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    print(f\"🔽 Downloading audio from YouTube: {youtube_url}\")\n",
    "    command = [\n",
    "        \"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\",\n",
    "        \"--output\", output_path, youtube_url\n",
    "    ]\n",
    "\n",
    "    subprocess.run(command, check=True)\n",
    "    print(f\"🎧 Audio downloaded and saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare and Trim Audio\n",
    "# ---------------------------\n",
    "def prepare_audio(input_path):\n",
    "    print(\"🎚️  Preparing audio (mono + 16kHz)...\")\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_channels(CHANNELS).set_frame_rate(SAMPLE_RATE)\n",
    "\n",
    "    max_duration_ms = MAX_AUDIO_DURATION_MIN * 60 * 1000\n",
    "    if len(audio) > max_duration_ms:\n",
    "        print(\"⏱️ Trimming audio to 30 minutes max\")\n",
    "        return audio[:max_duration_ms]\n",
    "    return audio\n",
    "\n",
    "# ---------------------------\n",
    "# Normalize audio volume\n",
    "# ---------------------------\n",
    "def normalize_audio(audio_segment, target_dBFS=TARGET_DBFS):\n",
    "    change = target_dBFS - audio_segment.dBFS\n",
    "    return audio_segment.apply_gain(change)\n",
    "\n",
    "# ---------------------------\n",
    "# Slice audio into raw frames for VAD\n",
    "# ---------------------------\n",
    "def make_frames(audio_segment, sample_rate, frame_duration_ms):\n",
    "    frame_len = int(sample_rate * frame_duration_ms / 1000.0) * 2\n",
    "    audio_bytes = audio_segment.raw_data\n",
    "    frames = []\n",
    "    for i in range(0, len(audio_bytes), frame_len):\n",
    "        frame = audio_bytes[i:i + frame_len]\n",
    "        if len(frame) == frame_len:\n",
    "            timestamp = int(i / (sample_rate * 2) * 1000)\n",
    "            frames.append((timestamp, frame))\n",
    "    return frames\n",
    "\n",
    "# ---------------------------\n",
    "# Detect voiced segments with WebRTC VAD\n",
    "# ---------------------------\n",
    "def vad_collector(audio_segment, sample_rate=SAMPLE_RATE, chunk_ms=30, vad_mode=VAD_MODE):\n",
    "    print(\"🗣️  Detecting voiced segments using WebRTC VAD...\")\n",
    "    vad = webrtcvad.Vad(vad_mode)\n",
    "    frames = make_frames(audio_segment, sample_rate, chunk_ms)\n",
    "    segments = []\n",
    "    voiced = []\n",
    "\n",
    "    for i, (timestamp, frame) in enumerate(frames):\n",
    "        is_speech = vad.is_speech(frame, sample_rate)\n",
    "        if is_speech:\n",
    "            voiced.append((timestamp, frame))\n",
    "        elif voiced:\n",
    "            start_ms = frames[i - len(voiced)][0]\n",
    "            end_ms = timestamp\n",
    "\n",
    "            # Apply padding around detected speech\n",
    "            chunk_start = max(0, start_ms - PADDING_MS)\n",
    "            chunk_end = min(len(audio_segment), end_ms + PADDING_MS)\n",
    "            chunk = audio_segment[chunk_start:chunk_end]\n",
    "\n",
    "            if len(chunk) >= CHUNK_DURATION_MS:\n",
    "                segments.append((chunk[:CHUNK_DURATION_MS], chunk_start, chunk_end))\n",
    "                print(f\"🎙️  Segment extracted: {chunk_start}ms → {chunk_end}ms\")\n",
    "\n",
    "            voiced = []\n",
    "\n",
    "    print(f\"✅ VAD found {len(segments)} voiced segments\")\n",
    "    return segments\n",
    "\n",
    "# ---------------------------\n",
    "# Transcribe segments and write to metadata\n",
    "# ---------------------------\n",
    "def transcribe_and_save(segments, output_dir, dialect, source_url, quota=100):\n",
    "    print(f\"📝 Transcribing and saving segments for dialect: {dialect}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, f\"{dialect}_metadata.csv\")\n",
    "\n",
    "    with open(metadata_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"sample_id\", \"filename\", \"dialect\", \"duration\", \"source_url\",\n",
    "            \"start_time_ms\", \"end_time_ms\", \"language\", \"avg_logprob\",\n",
    "            \"transcription\", \"whisper_model\"\n",
    "        ])\n",
    "\n",
    "        count = 0\n",
    "        random.shuffle(segments)\n",
    "\n",
    "        for seg, start, end in segments:\n",
    "            if count >= quota:\n",
    "                break\n",
    "            \n",
    "            seg = seg.low_pass_filter(3400).high_pass_filter(300)  # ✅ Bandpass filtering\n",
    "            seg = normalize_audio(seg)\n",
    "\n",
    "\n",
    "            seg = normalize_audio(seg)\n",
    "            temp_path = os.path.join(output_dir, \"temp.wav\")\n",
    "            seg.export(temp_path, format=\"wav\")\n",
    "\n",
    "            result = model.transcribe(temp_path, language=\"ar\", fp16=False)\n",
    "            transcript = result[\"text\"].strip()\n",
    "            language = result.get(\"language\", \"\")\n",
    "            avg_logprob = result.get(\"avg_logprob\", -10.0)\n",
    "\n",
    "            if transcript and language == \"ar\" and avg_logprob > -1.0:\n",
    "                filename = f\"{dialect}_chunk_{uuid.uuid4().hex[:8]}.wav\"\n",
    "                final_path = os.path.join(output_dir, filename)\n",
    "                seg.export(final_path, format=\"wav\")\n",
    "\n",
    "                sample_id = uuid.uuid4().hex[:12]\n",
    "                writer.writerow([\n",
    "                    sample_id, filename, dialect, round(seg.duration_seconds, 2),\n",
    "                    source_url, start, end, language, round(avg_logprob, 3),\n",
    "                    transcript, WHISPER_MODEL_SIZE\n",
    "                ])\n",
    "                print(f\"✅ [{count+1}] Saved: {filename} | 🗣️ {transcript}\")\n",
    "                count += 1\n",
    "            else:\n",
    "                print(\"❌ Skipped low-quality or non-Arabic sample\")\n",
    "\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    print(f\"📁 Metadata written to: {metadata_path}\")\n",
    "    print(f\"🎉 Total usable segments saved: {count}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline: Multiple videos per dialect\n",
    "# ---------------------------\n",
    "process_multiple_youtube_links(dialect, eval(f\"{dialect.lower()}_links\"), quota=10):\n",
    "\n",
    "print(f\"\\n🌍 Starting dataset build for: {dialect}\")\n",
    "dialect_dir = os.path.join(OUTPUT_BASE, dialect)\n",
    "os.makedirs(dialect_dir, exist_ok=True)\n",
    "\n",
    "all_segments = []\n",
    "quota_per_link = int(quota * 1.5)  # Over-sample, filter later\n",
    "\n",
    "for i, url in enumerate(links):\n",
    "    print(f\"\\n🔗 Processing video {i+1}/{len(links)}: {url}\")\n",
    "    try:\n",
    "            audio_path = download_youtube_audio(url, dialect_dir, f\"audio_{i+1}.mp3\")\n",
    "            audio = prepare_audio(audio_path)\n",
    "            segments = vad_collector(audio)\n",
    "            for seg in segments:\n",
    "                all_segments.append((*seg, url))  # save URL too\n",
    "    except Exception as e:\n",
    "            print(f\"⚠️ Error with link {url} — {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_segments:\n",
    "        print(f\"❌ No valid segments found for {dialect}\")\n",
    "        return\n",
    "\n",
    "    print(f\"🧮 Total segments collected: {len(all_segments)}\")\n",
    "    selected_segments = all_segments[:quota * 2]\n",
    "\n",
    "    # Drop URL (we already passed \"multiple\" as source_url for now)\n",
    "    final_segments = [(seg, start, end) for seg, start, end, _ in selected_segments]\n",
    "\n",
    "    transcribe_and_save(final_segments, dialect_dir, dialect, source_url=\"multiple\", quota=quota)\n",
    "    print(f\"🏁 Finished dialect: {dialect}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabc078",
   "metadata": {},
   "source": [
    "# Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a966d8",
   "metadata": {},
   "source": [
    "\n",
    "This notebook contains YouTube audio collection and cleaning blocks for Arabic dialects:\n",
    "- Lebanese\n",
    "- Egyptian\n",
    "- Syrian\n",
    "- Palestinian\n",
    "- Jordanian\n",
    "- Iraqi\n",
    "- Saudi\n",
    "- Emirati\n",
    "\n",
    "Select the block for the dialect you want to process and then run the final pipeline cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b7ab4",
   "metadata": {},
   "source": [
    "### Lebanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38af690-6535-4d9f-8d83-23128eed06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Lebanese\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=QALZXfprao4\",\n",
    "    \"https://www.youtube.com/watch?v=ni0_JIhc1h4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda39a5e",
   "metadata": {},
   "source": [
    "### Jordanian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7adfa7-3c3a-48ec-b550-32f89999be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Jordanian\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba913d9",
   "metadata": {},
   "source": [
    "### Palestinian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f967c5-a29d-47cc-850e-43b35df42bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Palestinian\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f32673",
   "metadata": {},
   "source": [
    "### Syrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect = \"Syrian\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e92a9f",
   "metadata": {},
   "source": [
    "### Saudi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd116b3-b271-46dd-88d1-5dc0cc8a9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Saudi\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75729e9f",
   "metadata": {},
   "source": [
    "### Egyptian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0a81b-fa3d-4993-8ada-183aa670b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Egyptian\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b879ee",
   "metadata": {},
   "source": [
    "### Emarati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29939ae7-3e84-40a6-bd4f-9958379cf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Emarati\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802199d",
   "metadata": {},
   "source": [
    "### Iraqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8248f-682c-4878-b015-e5a8de9fb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialect = \"Iraqi\"\n",
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=wLZ5TkkJyzI\",\n",
    "    \"https://www.youtube.com/watch?v=example2\",\n",
    "    \"https://www.youtube.com/watch?v=example3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4cbaf",
   "metadata": {},
   "source": [
    "# Dataset Splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c07ee6",
   "metadata": {},
   "source": [
    "##### 🔀 Dataset Splitting: Train / Validation / Test\n",
    "\n",
    "After collecting and cleaning audio samples for each dialect, we split the data into:\n",
    "- **Training set** (`train/`): used to train the model\n",
    "- **Validation set** (`val/`): used during training to check performance\n",
    "- **Test set** (`test/`): used after training to evaluate final accuracy\n",
    "\n",
    "Each split gets its own folder with:\n",
    "- Clean `.wav` audio files\n",
    "- A corresponding `mel/` folder for Mel spectrograms\n",
    "\n",
    "This process also updates the metadata file to include a new column: `\"split\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca03797-5a53-4d2b-9341-6d225763dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(dialect_dir, dialect, test_size=0.15, val_size=0.15):\n",
    "    print(f\"📂 Splitting dataset for: {dialect}\")\n",
    "\n",
    "    df = pd.read_csv(f\"{dialect_dir}/{dialect}_metadata.csv\")\n",
    "\n",
    "    train_val, test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    df['split'] = 'train'\n",
    "    df.loc[val.index, 'split'] = 'val'\n",
    "    df.loc[test.index, 'split'] = 'test'\n",
    "\n",
    "    # Create folders\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for sub in ['wav', 'mel']:\n",
    "            os.makedirs(os.path.join(dialect_dir, split, sub), exist_ok=True)\n",
    "\n",
    "    # Move files to new folders\n",
    "    for _, row in df.iterrows():\n",
    "        src = os.path.join(dialect_dir, row['filename'])\n",
    "        dst = os.path.join(dialect_dir, row['split'], 'wav', row['filename'])\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "    df.to_csv(os.path.join(dialect_dir, f\"{dialect}_metadata.csv\"), index=False)\n",
    "    print(f\"✅ Done splitting {len(df)} files into train/val/test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071b576",
   "metadata": {},
   "source": [
    "# 🎼 Generate Mel Spectrograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c7cf0",
   "metadata": {},
   "source": [
    "\n",
    "For each audio file, we generate a Mel spectrogram and save it as a `.png` image.\n",
    "\n",
    "This helps transform raw `.wav` files into a visual format for training CNN models.\n",
    "\n",
    "After generation:\n",
    "- Spectrograms are stored in `mel/` folders inside each split (`train`, `val`, `test`)\n",
    "- Their paths are recorded in the metadata under the `\"mel_path\"` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070cbc9-9216-4719-a21a-8519e48f0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_mel_spectrograms(dialect_dir, dialect):\n",
    "    print(f\"🎼 Generating Mel spectrograms for: {dialect}\")\n",
    "    df = pd.read_csv(os.path.join(dialect_dir, f\"{dialect}_metadata.csv\"))\n",
    "    df['mel_path'] = \"\"\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        wav_path = os.path.join(dialect_dir, row['split'], 'wav', row['filename'])\n",
    "        mel_path = os.path.join(dialect_dir, row['split'], 'mel', row['filename'].replace('.wav', '.png'))\n",
    "\n",
    "        try:\n",
    "            y, sr = librosa.load(wav_path, sr=16000)\n",
    "            mel = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            librosa.display.specshow(mel_db, sr=sr, x_axis=None, y_axis=None)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(mel_path, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "            df.at[i, 'mel_path'] = mel_path\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error on {row['filename']} → {e}\")\n",
    "    \n",
    "        df.to_csv(os.path.join(dialect_dir, f\"{dialect}_metadata.csv\"), index=False)\n",
    "        print(f\"✅ Mel spectrograms saved and metadata updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
